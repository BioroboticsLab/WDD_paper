{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import itertools\n",
    "from os.path import isfile\n",
    "from glob import glob\n",
    "import h5py\n",
    "from matplotlib import pyplot as plt\n",
    "import re\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "from skimage import io\n",
    "from skimage.color import rgb2gray\n",
    "import pandas as pd\n",
    "from tqdm import trange, tqdm\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_paths = [\n",
    "    '/mnt/storage/beesbook-data-clean/machine_learning/WDD/WDD_GroundTruth_2016/',\n",
    "    '/mnt/storage/beesbook-data-clean/projects/adrian_dancedetection/data/20160816_data/20160816/'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x1 = list(itertools.chain.from_iterable(map(lambda path: glob(path + \"*/*/\"), image_paths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x2 = np.zeros((len(x1),)).astype(int)\n",
    "for i, x in enumerate(x1):\n",
    "    x2[i] = len(glob(x + '/image_*.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path_16 = '/mnt/storage/ben/data/BeesBook/AdeferDanceDetection/20170620_1606_FE.csv'\n",
    "gt_data_16 = pd.read_csv(csv_path_16, header=None, names=['key', 'gt_intern', 'gt'], index_col=0)\n",
    "gt_data_16 = gt_data_16[~gt_data_16.index.duplicated(keep='first')]\n",
    "gt_data_16.drop('gt_intern', axis=1, inplace=True)\n",
    "gt_data_16.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path_14 = '/mnt/storage/ben/data/BeesBook/AdeferDanceDetection/GT_20160816.csv'\n",
    "gt_data_14 = pd.read_csv(csv_path_14, header=None, names=['key', 'gt'], index_col=0)\n",
    "gt_data_14 = gt_data_14[~gt_data_14.index.duplicated(keep='first')]\n",
    "gt_data_14.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_data = pd.concat((gt_data_14, gt_data_16), axis=0)\n",
    "gt_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X1 = []\n",
    "X2 = []\n",
    "Y = []\n",
    "for (i, path), nimages in zip(enumerate(x1), x2):\n",
    "    try:\n",
    "        label = int(gt_data.loc['/'.join(path.split('/')[-3:-1])]['gt'])\n",
    "        X1.append(path)\n",
    "        X2.append(nimages)\n",
    "        Y.append(label)\n",
    "    except KeyError:\n",
    "        continue\n",
    "\n",
    "x1 = np.array(X1)\n",
    "x2 = np.array(X2)\n",
    "y = np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gt_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filter_data_path = '/mnt/storage/ben/data/BeesBook/AdeferDanceDetection/filter_data_rec_mixed.h5'\n",
    "\n",
    "if not isfile(filter_data_path):\n",
    "    with h5py.File(filter_data_path, 'w') as f:\n",
    "        g_X = f.create_group('X')\n",
    "        g_y = f.create_dataset('y', (len(y), ), np.int64)\n",
    "        f.create_dataset('n_samples', data=[len(y)])\n",
    "\n",
    "        for idx in trange(len(y)):\n",
    "            n_samples = x2[idx]\n",
    "\n",
    "            samples = []\n",
    "            for i in range(n_samples):\n",
    "                image_path = x1[idx] + \"image_\" + str(i).zfill(3) + \".png\"\n",
    "                samples.append(rgb2gray(io.imread(image_path)))\n",
    "\n",
    "            samples = np.swapaxes(np.swapaxes(np.array(samples), 0, 2), 0,1)[np.newaxis]\n",
    "            g_X.create_dataset('{}'.format(idx), data=samples.astype(np.float32))\n",
    "            g_y[idx] = y[idx] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(filter_data_path, 'r') as f:\n",
    "    n_samples = f['n_samples'][0]\n",
    "    g_X = f['X']\n",
    "    seq_lens = []\n",
    "    \n",
    "    for idx in trange(n_samples):\n",
    "        seq_lens.append(g_X['{}'.format(idx)].shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(seq_lens)\n",
    "print(np.max(seq_lens))\n",
    "print(np.min(seq_lens))\n",
    "print(np.median(seq_lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indices = list(range(len(seq_lens)))\n",
    "np.random.shuffle(indices)\n",
    "train_indices = indices[:int(.8 * len(indices))]\n",
    "val_indices = indices[int(.8 * len(indices)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_gen(data, indices, batch_size, seq_len=128, augment=False, random_border_crop=4, cutoff=10):\n",
    "    assert(random_border_crop % 2 == 0)\n",
    "    X = data['X']\n",
    "    y = data['y']\n",
    "    width = X['0'].shape[1]\n",
    "    heigth = X['0'].shape[2]\n",
    "    \n",
    "    while True:\n",
    "        batch_indices = np.random.choice(indices, replace=False, size=batch_size)\n",
    "        batch_samples = []\n",
    "        batch_labels = []\n",
    "        \n",
    "        for idx in batch_indices:\n",
    "            sample = X['{}'.format(idx)][:]\n",
    "            sample = sample[:, :, :, cutoff:-cutoff]\n",
    "            assert(sample.shape[-1] > 10)\n",
    "            if sample.shape[-1] > seq_len:\n",
    "                start_idx = np.random.randint(0, sample.shape[-1] - seq_len)\n",
    "                sample = sample[:, :, :, start_idx:start_idx+seq_len]\n",
    "            batch_samples.append(sample)\n",
    "            batch_labels.append(y[idx])\n",
    "            \n",
    "        X_b = np.zeros((batch_size, 1, width, heigth, seq_len), dtype=np.float32)\n",
    "        for idx, sample in enumerate(batch_samples):\n",
    "            X_b[idx, :, :, :, :sample.shape[-1]] = sample\n",
    "            \n",
    "        if augment:\n",
    "            crops = np.random.randint(0, random_border_crop, size=(X_b.shape[0], 2))\n",
    "            crops = [(slice(None, None, None),\n",
    "                      slice(crops[i, 0], -random_border_crop+crops[i, 0], None),\n",
    "                      slice(crops[i, 1], -random_border_crop+crops[i, 1], None),\n",
    "                      slice(None, None, None))\n",
    "                      for i in range(X_b.shape[0])]\n",
    "            flips = [(slice(None, None, None),\n",
    "                      slice(None, None, random.choice([-1, None])),\n",
    "                      slice(None, None, random.choice([-1, None])),\n",
    "                      slice(None, None, None))\n",
    "                      for i in range(X_b.shape[0])]\n",
    "            X_b = np.array([(seq[crop])[flip] for seq, crop, flip in zip(X_b, crops, flips)])\n",
    "        else:\n",
    "            crop = random_border_crop // 2\n",
    "            X_b = X_b[:, :, crop:-crop, crop:-crop, :]\n",
    "            \n",
    "        yield X_b, np.array(batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution3D, MaxPooling3D\n",
    "from keras.optimizers import Adam, Nadam\n",
    "from keras.utils.io_utils import HDF5Matrix\n",
    "from keras import backend as K\n",
    "from keras.layers.convolutional import Conv3D, AveragePooling3D\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AdamWithWeightnorm(Adam):\n",
    "    def get_updates(self, params, constraints, loss):\n",
    "        grads = self.get_gradients(loss, params)\n",
    "        self.updates = [K.update_add(self.iterations, 1)]\n",
    "\n",
    "        lr = self.lr\n",
    "        if self.initial_decay > 0:\n",
    "            lr *= (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "        t = self.iterations + 1\n",
    "        lr_t = lr * K.sqrt(1. - K.pow(self.beta_2, t)) / (1. - K.pow(self.beta_1, t))\n",
    "\n",
    "        shapes = [K.get_variable_shape(p) for p in params]\n",
    "        ms = [K.zeros(shape) for shape in shapes]\n",
    "        vs = [K.zeros(shape) for shape in shapes]\n",
    "        self.weights = [self.iterations] + ms + vs\n",
    "\n",
    "        for p, g, m, v in zip(params, grads, ms, vs):\n",
    "\n",
    "            # if a weight tensor (len > 1) use weight normalized parameterization\n",
    "            # this is the only part changed w.r.t. keras.optimizers.Adam\n",
    "            ps = K.get_variable_shape(p)\n",
    "            if len(ps)>1:\n",
    "\n",
    "                # get weight normalization parameters\n",
    "                V, V_norm, V_scaler, g_param, grad_g, grad_V = get_weightnorm_params_and_grads(p, g)\n",
    "\n",
    "                # Adam containers for the 'g' parameter\n",
    "                V_scaler_shape = K.get_variable_shape(V_scaler)\n",
    "                m_g = K.zeros(V_scaler_shape)\n",
    "                v_g = K.zeros(V_scaler_shape)\n",
    "\n",
    "                # update g parameters\n",
    "                m_g_t = (self.beta_1 * m_g) + (1. - self.beta_1) * grad_g\n",
    "                v_g_t = (self.beta_2 * v_g) + (1. - self.beta_2) * K.square(grad_g)\n",
    "                new_g_param = g_param - lr_t * m_g_t / (K.sqrt(v_g_t) + self.epsilon)\n",
    "                self.updates.append(K.update(m_g, m_g_t))\n",
    "                self.updates.append(K.update(v_g, v_g_t))\n",
    "\n",
    "                # update V parameters\n",
    "                m_t = (self.beta_1 * m) + (1. - self.beta_1) * grad_V\n",
    "                v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(grad_V)\n",
    "                new_V_param = V - lr_t * m_t / (K.sqrt(v_t) + self.epsilon)\n",
    "                self.updates.append(K.update(m, m_t))\n",
    "                self.updates.append(K.update(v, v_t))\n",
    "\n",
    "                # if there are constraints we apply them to V, not W\n",
    "                if p in constraints:\n",
    "                    c = constraints[p]\n",
    "                    new_V_param = c(new_V_param)\n",
    "\n",
    "                # wn param updates --> W updates\n",
    "                add_weightnorm_param_updates(self.updates, new_V_param, new_g_param, p, V_scaler)\n",
    "\n",
    "            else: # do optimization normally\n",
    "                m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n",
    "                v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(g)\n",
    "                p_t = p - lr_t * m_t / (K.sqrt(v_t) + self.epsilon)\n",
    "\n",
    "                self.updates.append(K.update(m, m_t))\n",
    "                self.updates.append(K.update(v, v_t))\n",
    "\n",
    "                new_p = p_t\n",
    "                # apply constraints\n",
    "                if p in constraints:\n",
    "                    c = constraints[p]\n",
    "                    new_p = c(new_p)\n",
    "                self.updates.append(K.update(p, new_p))\n",
    "        return self.updates\n",
    "\n",
    "\n",
    "def get_weightnorm_params_and_grads(p, g):\n",
    "    ps = K.get_variable_shape(p)\n",
    "\n",
    "    # construct weight scaler: V_scaler = g/||V||\n",
    "    V_scaler_shape = (ps[-1],)  # assumes we're using tensorflow!\n",
    "    V_scaler = K.ones(V_scaler_shape)  # init to ones, so effective parameters don't change\n",
    "\n",
    "    # get V parameters = ||V||/g * W\n",
    "    norm_axes = [i for i in range(len(ps) - 1)]\n",
    "    V = p / tf.reshape(V_scaler, [1] * len(norm_axes) + [-1])\n",
    "\n",
    "    # split V_scaler into ||V|| and g parameters\n",
    "    V_norm = tf.sqrt(tf.reduce_sum(tf.square(V), norm_axes))\n",
    "    g_param = V_scaler * V_norm\n",
    "\n",
    "    # get grad in V,g parameters\n",
    "    grad_g = tf.reduce_sum(g * V, norm_axes) / V_norm\n",
    "    grad_V = tf.reshape(V_scaler, [1] * len(norm_axes) + [-1]) * \\\n",
    "             (g - tf.reshape(grad_g / V_norm, [1] * len(norm_axes) + [-1]) * V)\n",
    "\n",
    "    return V, V_norm, V_scaler, g_param, grad_g, grad_V\n",
    "\n",
    "\n",
    "def add_weightnorm_param_updates(updates, new_V_param, new_g_param, W, V_scaler):\n",
    "    ps = K.get_variable_shape(new_V_param)\n",
    "    norm_axes = [i for i in range(len(ps) - 1)]\n",
    "\n",
    "    # update W and V_scaler\n",
    "    new_V_norm = tf.sqrt(tf.reduce_sum(tf.square(new_V_param), norm_axes))\n",
    "    new_V_scaler = new_g_param / new_V_norm\n",
    "    new_W = tf.reshape(new_V_scaler, [1] * len(norm_axes) + [-1]) * new_V_param\n",
    "    updates.append(K.update(W, new_W))\n",
    "    updates.append(K.update(V_scaler, new_V_scaler))\n",
    "\n",
    "\n",
    "# data based initialization for a given Keras model\n",
    "def data_based_init(model, input):\n",
    "    # input can be dict, numpy array, or list of numpy arrays\n",
    "    if type(input) is dict:\n",
    "        feed_dict = input\n",
    "    elif type(input) is list:\n",
    "        feed_dict = {tf_inp: np_inp for tf_inp,np_inp in zip(model.inputs,input)}\n",
    "    else:\n",
    "        feed_dict = {model.inputs[0]: input}\n",
    "\n",
    "    # add learning phase if required\n",
    "    if model.uses_learning_phase and K.learning_phase() not in feed_dict:\n",
    "        feed_dict.update({K.learning_phase(): 1})\n",
    "\n",
    "    # get all layer name, output, weight, bias tuples\n",
    "    layer_output_weight_bias = []\n",
    "    for l in model.layers:\n",
    "        if hasattr(l, 'W') and hasattr(l, 'b'):\n",
    "            assert(l.built)\n",
    "            layer_output_weight_bias.append( (l.name,l.get_output_at(0),l.W,l.b) ) # if more than one node, only use the first\n",
    "\n",
    "    # iterate over our list and do data dependent init\n",
    "    sess = K.get_session()\n",
    "    for l,o,W,b in layer_output_weight_bias:\n",
    "        print('Performing data dependent initialization for layer ' + l)\n",
    "        m,v = tf.nn.moments(o, [i for i in range(len(o.get_shape())-1)])\n",
    "        s = tf.sqrt(v + 1e-10)\n",
    "        updates = tf.group(W.assign(W/tf.reshape(s,[1]*(len(W.get_shape())-1)+[-1])), b.assign((b-m)/s))\n",
    "        sess.run(updates, feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def selu(x):\n",
    "    \"\"\"Scaled Exponential Linear Unit. (Klambauer et al., 2017)\n",
    "    # Arguments\n",
    "        x: A tensor or variable to compute the activation function for.\n",
    "    # References\n",
    "        - [Self-Normalizing Neural Networks](https://arxiv.org/abs/1706.02515)\n",
    "    \"\"\"\n",
    "    alpha = 1.6732632423543772848170429916717\n",
    "    scale = 1.0507009873554804934193349852946\n",
    "    return scale * K.elu(x, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq_len = 128\n",
    "\n",
    "inputs = Input((1, 46, 46, seq_len))\n",
    "\n",
    "n_filters = 32\n",
    "l2 = 1e-8\n",
    "\n",
    "x = Permute((1, 4, 2, 3))(inputs)\n",
    "\n",
    "x = Conv3D(n_filters, 5, strides=(2, 2, 2), data_format='channels_first', activation=selu,\n",
    "           kernel_regularizer=regularizers.l2(l2), bias_regularizer=regularizers.l2(l2))(x)\n",
    "x = MaxPooling3D((1, 2, 2), data_format='channels_first')(x)\n",
    "\n",
    "x = Conv3D(n_filters * 2, 3, strides=(1, 1, 1), data_format='channels_first', activation=selu,\n",
    "           kernel_regularizer=regularizers.l2(l2), bias_regularizer=regularizers.l2(l2))(x)\n",
    "x = MaxPooling3D((2, 2, 2), data_format='channels_first')(x)\n",
    "\n",
    "x = Conv3D(n_filters * 4, 3, strides=(1, 1, 1), data_format='channels_first', activation=selu,\n",
    "           kernel_regularizer=regularizers.l2(l2), bias_regularizer=regularizers.l2(l2))(x)\n",
    "\n",
    "x = AveragePooling3D((28, 2, 2), data_format='channels_first')(x)\n",
    "x = Flatten()(x)\n",
    "\n",
    "x = Dropout(.2)(x)\n",
    "x = Dense(n_filters * 4, activation=selu,\n",
    "          kernel_regularizer=regularizers.l2(l2), bias_regularizer=regularizers.l2(l2))(x)\n",
    "\n",
    "x = Dense(3, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=AdamWithWeightnorm(0.0005), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_path = '/mnt/storage/ben/data/BeesBook/AdeferDanceDetection/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def schedule(epoch):\n",
    "    return 0.0005 if epoch < 100 else 0.0005 * .25\n",
    "\n",
    "scheduler = LearningRateScheduler(schedule)\n",
    "\n",
    "batch_size = 32\n",
    "train_gen = data_gen(h5py.File(filter_data_path, 'r'), train_indices, batch_size, augment=True)\n",
    "val_gen = data_gen(h5py.File(filter_data_path, 'r'), val_indices, batch_size)\n",
    "\n",
    "hist = model.fit_generator(train_gen, len(train_indices) // batch_size, 200, \n",
    "                           validation_data=val_gen, validation_steps=len(val_indices) // batch_size,\n",
    "                           callbacks=[scheduler])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
